# zookeeperServers is used for distributed training 
zookeeperServers=

# hadoopNumParallel is for parallelity for when using mapreduce mode
# deprecated: there is auto parallel setting in all pig scripts to help user no need to set this number
#hadoopNumParallel=40

# hadoopJobQueue specifies the hadoop job queue
hadoopJobQueue=default

# localNumParallel is for parallelity for when using local mode
localNumParallel=6

# how many records per message
recordCntPerMessage=100000

# fix a bug on hdp 2.4.1 by default mapreduce.job.max.split.locations is 10
mapreduce.job.max.split.locations=5000

# disable map output compress because some issue for snappy config in our clusters
# mapreduce.map.output.compress=false

# how to control how many mappers when training models
# If there are too many small files after normalization, set it to true
# It will try to merge multi small files in mapper and reduce the number of mappers
# guagua.split.combinable=true

# It will control the max combined split size for each mapper.
# Sometimes mapper will timeout for loading data, if there are too much data for mapper
# This option can limit the records for each mapper
# guagua.split.maxCombinedSplitSize=256000000

# set to 30mins (default 10mins)
mapreduce.task.timeout=1800000

# set default zk in client machine by default it is false and in master node, if set it to false, zk will be sstarted in 
# master task. Please tune memory to make sure enough memory in master zookeeper process.
guagua.zk.embedbed.isInClient=true

# if over such threshold three times, kill worker; by default is 60000, change it to 30 minutes to avoid failures
# set worker compute time threshold to 30 minutes, if compute time of 3 iterations >= this value, worker will kill itself.
guagua.computation.time.threshold=1800000

## tuning for rf, in rf, tree models are stored in master memory, if two many models, please tune such parameters 
mapreduce.map.memory.mb=2048
mapreduce.map.java.opts=-Xms1700m -Xmx1700m -server -XX:MaxPermSize=64m -XX:PermSize=64m -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:ParallelGCThreads=8 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps

#set io buffer in mapper to 400mb to avoid too many memory consumption in mapper heap
mapreduce.task.io.sort.mb=400

## each round how many jobs can be running, if bagging is 10, two rounds of bagging with each one 5 guagua jobs 
shifu.train.bagging.inparallel=5

## if number of hyper parameter composite over such threshold, random search will be enabled. 
shifu.gridsearch.threshold=30

## set min worker done ratio in each guagua iteration step 
## From 0.11.0, change to 0.95 to make it works well for # of workers less than 50
guagua.min.workers.ratio=0.95

## the hadoop capacity may limit parallel number, add this parameter for hadoop performance tunning
# shifu.combo.thread.parallel = 5

## the max retry times for when executing combo jobs
# shifu.combo.max.retry = 3

## control the behavior of normalized data
# how many parts of normalized data that will generate
# shifu.norm.shuffle.size = 100
# the preferred normalized data size per part
# shifu.norm.prefer.part.size = 256000000

# the precision of model scores
# shifu.score.scale = 1000

## enable pig combination to save mappers
pig.splitCombination=true
pig.maxCombinedSplitSize=268435456

## specified reduce memory to avoid default memory too big or too small
mapreduce.reduce.memory.mb=2048
mapreduce.reduce.java.opts=-Xms1700m -Xmx1700m -server -XX:MaxPermSize=64m -XX:PermSize=64m

# namespace support - default disable
shifu.namespace.strict.mode = false

# add worker get result waiting timeout to 10 minutes
guagua.worker.getresult.timeout=600000


